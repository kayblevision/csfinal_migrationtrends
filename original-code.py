# -*- coding: utf-8 -*-
"""CS7510 Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jMUjzskrgrh7cZZJ6pghzhBhkJbX34i6
"""

#!pip3 install cython~=0.22
#!pip3 install numpy

#!pip3 install pystan~=2.14

#!pip3 install prophet

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import plotly
import plotly.express as px
import seaborn as sns

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

from prophet import Prophet
from sklearn.metrics import mean_squared_error, mean_absolute_error

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/time_series.csv')

df.head(20)

df.info()

"""seems there are non-numeric values in the Value column --> will convert and have to drop NAs most likely"""

df['Value'] = pd.to_numeric(df['Value'], errors='coerce')
#df['Value'] = df['Value'].astype('int64')

"""Just testing something out with dfn. can ignore that subset"""

dfn = df[df['Value'].isna()]

dfn['Country / territory of asylum/residence']

dfn['Country / territory of asylum/residence'].value_counts()

df['Country / territory of asylum/residence'].value_counts()

"""which years have the greatest number of NA values?"""

dfn['Year'].value_counts()

"""Seems like it's just 2016 data that's missing, which ultimately is fine. I will just take out any remaining 2016 data from the main dataframe so that our dataset is just 1951-2015"""

df = df.dropna(subset=['Value'])

df[df['Year'] == 2016]

"""So the 2016 data remaining in the original df is over 43000 rows which does not need to be deleted.

Now just getting a feel for what the other columns look like:
"""

df['Population type'].value_counts()

df.describe()

df.boxplot(column='Value', vert=False)
plt.xlabel('Data')
plt.ylabel('Value')
plt.title('Boxplot of Value data')
plt.show()

"""No dropping of outliers

Doing the interactive plots here:
"""

resettlement = pd.read_csv('/content/resettlement.csv')
resettlement.info()

my_list = resettlement['Country / territory of asylum/residence']

unique_values = list(set(my_list))
print(unique_values)

# Mapping country names to ISO3 codes
iso3_mapping = {
    'Australia': 'AUS',
    'Germany': 'DEU',
    'Czech Rep.': 'CZE',
    'Lithuania': 'LTU',
    'Austria': 'AUT',
    'Latvia': 'LVA',
    'Iceland': 'ISL',
    'Burundi': 'BDI',
    'Cameroon': 'CMR',
    'Ireland': 'IRL',
    'France': 'FRA',
    'Paraguay': 'PRY',
    'Hungary': 'HUN',
    'Norway': 'NOR',
    'Mexico': 'MEX',
    'Brazil': 'BRA',
    'Liechtenstein': 'LIE',
    'Luxembourg': 'LUX',
    'Belgium': 'BEL',
    'Denmark': 'DNK',
    'Romania': 'ROU',
    'Iran (Islamic Rep. of)': 'IRN',
    'Benin': 'BEN',
    'Italy': 'ITA',
    'Monaco': 'MCO',
    'Switzerland': 'CHE',
    'New Zealand': 'NZL',
    'Belarus': 'BLR',
    'Burkina Faso': 'BFA',
    'Finland': 'FIN',
    'Netherlands': 'NLD',
    'El Salvador': 'SLV',
    'Nicaragua': 'NIC',
    'Philippines': 'PHL',
    'Spain': 'ESP',
    'Uruguay': 'URY',
    'Cambodia': 'KHM',
    'Rep. of Korea': 'KOR',
    'Jordan': 'JOR',
    'Ecuador': 'ECU',
    'United Kingdom of Great Britain and Northern Ireland': 'GBR',
    'Japan': 'JPN',
    'Canada': 'CAN',
    'Chile': 'CHL',
    'Portugal': 'PRT',
    'United States of America': 'USA',
    'Guatemala': 'GTM',
    'Yemen': 'YEM',
    'South Africa': 'ZAF',
    'Sweden': 'SWE',
    'Central African Rep.': 'CAF',
    'Argentina': 'ARG',
    'Estonia': 'EST'
}

# chatgpt

# Assuming 'resettlement' is your DataFrame with columns 'residence' and 'Value'
# And 'countrycode_data' contains country code information with columns 'country.name.en' and 'iso3c'

# Convert 'Value' column to numeric in the resettlement DataFrame
resettlement['Value'] = pd.to_numeric(resettlement['Value'], errors='coerce')

# Group by 'residence' and calculate total resettlement values
countries_r = resettlement.groupby('Country / territory of asylum/residence')['Value'].sum().reset_index()

# Mapping country names to ISO3 codes
#countries_r.loc[countries_r.index.isin(iso3_mapping.values()), 'iso3'] = iso3_mapping.keys()

countries_r['iso3'] = countries_r['Country / territory of asylum/residence'].map(iso3_mapping)

fig = px.choropleth(countries_r,
                    locations='iso3',
                    color='Value',
                    hover_name='Country / territory of asylum/residence',
                    projection='natural earth',
                    title='Resettlement of asylum seekers by country of residence',
                    color_continuous_scale='viridis',
                    range_color=[0, countries_r['Value'].max()],
                    labels={'Value': 'Total Resettlement'}
                    )

#fig.update_geos(fitbounds="locations", visible=False)
#fig.update_layout(showlegend=False)
fig.show()

plotly.offline.plot(fig, filename='world_map.html', auto_open=True)

"""Might be worth it to look at Afghanistan and Syria specifically over time"""

asm = pd.read_csv('/content/asylum_seekers_monthly.csv')
asm.head()

asm.info()

"""crucial piece of code to run (below). Without it the Value column is not numeric"""

asm['Value'] = pd.to_numeric(asm['Value'], errors='coerce')
asm = asm.dropna(subset=['Value'])

asm.boxplot(column='Value', vert=False)
plt.xlabel('Data')
plt.ylabel('Value')
plt.title('Boxplot of Value data')
plt.show()

"""visualizing with the ASM dataset"""

# Got help from chatgpt to translate this from R

# Filter data for Afghanistan and Syrian Arab Rep.
filtered_data = asm[(asm['Origin'] == "Afghanistan") | (asm['Origin'] == "Syrian Arab Rep.")]

# Select columns 'Origin', 'Year', 'Month', 'Value'
selected_data = filtered_data[['Origin', 'Year', 'Month', 'Value']]

# Group by 'Origin' and 'Year', and calculate the sum of 'Value'
grouped_data = selected_data.groupby(['Origin', 'Year']).agg(Total=('Value', 'sum')).reset_index()

# Filtered and summarized data for Afghanistan and Syrian Arab Rep.
af_S_tots = grouped_data

# Create an interactive line plot using Plotly
fig = px.line(af_S_tots, x='Year', y='Total', color='Origin',
              title='Origination of Refugees: AFGHANISTAN - SYRIA: 1999 - 2016',
              labels={'Year': 'Year', 'Total': 'Total', 'Origin': 'Origin'},
              template='plotly_white')

# Update layout and display the plot
fig.update_layout(
    title='Refugees from Afghanistan and Syria: 1999 - 2016',
    xaxis_title='Year',
    yaxis_title='Total',
    legend_title='Origin',
    hovermode='x unified',
    hoverlabel=dict(bgcolor="white", font_size=12)
)

fig.show()

"""Uptick possibly due to period of war"""

# exporting the plot to keep the interactivity
plotly.offline.plot(fig, filename='af_s.html', auto_open=True)

#import pandas as pd
#import plotly.express as px

# Grouping by 'residence' and calculating the sum of 'Value'
applications_r = asm.groupby('Country / territory of asylum/residence').agg(Total=('Value', 'sum')).reset_index()
applications_r = applications_r.sort_values(by='Total', ascending=False)

# Creating a bar chart using plotly
fig = px.bar(applications_r, x='Country / territory of asylum/residence', y='Total',
             title='Applications received by country of residence',
             labels={'Country / territory of asylum/residence': 'Country of Residence', 'Total': 'No. of Applications'})

# Customizing the layout
fig.update_layout(xaxis_title='Country of Residence', yaxis_title='No. of Applications',
                  title_x=0.5, bargap=0.2)

# Adding credits
fig.update_layout(annotations=[dict(text="Data Source: UNHCR", showarrow=False,
                                    xref="paper", yref="paper", x=0.99, y=0)])

# Removing legend
fig.update_layout(showlegend=False)

# Displaying the chart
fig.show()

plotly.offline.plot(fig, filename='app_res.html', auto_open=True)

# Applications by residence country per year
# I did most of the work, chatgpt helped a little

# Filtering data for specific countries
selected_countries = ["Canada", "USA (INS/DHS)", "Australia"]
res_year = asm[asm['Country / territory of asylum/residence'].isin(selected_countries)]

# Grouping and aggregating data
applications_r = res_year.groupby(['Year', 'Country / territory of asylum/residence']).agg(Total=('Value', 'sum')).reset_index()
applications_r = applications_r.sort_values(by='Total', ascending=False)

# Creating a color map for countries
colors = {'Canada': 'white', 'USA (INS/DHS)': 'darkblue', 'Australia': 'crimson'}

# Creating a bar chart using plotly
fig = px.bar(applications_r, x='Year', y='Total', color='Country / territory of asylum/residence',
             title='Applications received by top three resettlement countries by year',
             labels={'Year': 'Year', 'Total': 'No. of Applications'},
            color_discrete_map=colors)

# Customizing the layout
fig.update_layout(xaxis_title='Year', yaxis_title='No. of Applications',
                  title_x=0.5, bargap=0.2)

# Adding credits
fig.update_layout(annotations=[dict(text="Data Source: UNHCR", showarrow=False,
                                    xref="paper", yref="paper", x=0.99, y=0)])

# Adding labels to bars
#fig.update_traces(text=applications_r['Country / territory of asylum/residence'],
               #   textposition='inside')

# Displaying the chart
fig.show()

plotly.offline.plot(fig, filename='app_res_year.html', auto_open=True)

"""Preparing the data for modeling"""

unique_values_sorted = sorted(df['Country / territory of asylum/residence'].unique())

# Factorize values based on alphabetical order
#df['Country / territory of asylum/residence'] = pd.Categorical(df['Country / territory of asylum/residence'], categories=unique_values_sorted).codes

df.head()

"""I used yearly data at first and the modeling didn't go well. Going to try using monthly data to see if it fits the model better"""

filtered_data1 = asm[(asm['Country / territory of asylum/residence'] == "Canada")] #| (asm['Origin'] == "Syrian Arab Rep.")]

# Select columns 'Origin', 'Year', 'Month', 'Value'
selected_data1 = filtered_data1[['Origin', 'Year', 'Month', 'Value']]

# Group by 'Origin' and 'Year', and calculate the sum of 'Value'
grouped_data1 = selected_data1.groupby(['Year', 'Month']).agg(Total=('Value', 'sum')).reset_index() # sum up the months to represent the year
#grouped_data1 = selected_data1.groupby(['Year']).agg(Total=('Value', 'sum')).reset_index() # then sum all the countries respective values for that year
grouped_data1.shape

# Define a dictionary to map month names to their numerical representation
month_map = {
    'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5,
    'June': 6, 'July': 7, 'August': 8, 'September': 9, 'October': 10,
    'November': 11, 'December': 12
}

# Convert month names to their numerical representation
grouped_data1['Month_Num'] = grouped_data1['Month'].map(month_map)

# Combine 'Year' and 'Month_Num' columns to create a new column representing the first day of each month
grouped_data1['First_Day_of_Month'] = pd.to_datetime(grouped_data1['Year'].astype(str) + '-' + grouped_data1['Month_Num'].astype(str) + '-01')

grouped_data1.head()
grouped_data1 = grouped_data1.rename(columns={'First_Day_of_Month': "ds", "Total" : "y"})

#canada.head()

canada = grouped_data1[['ds','y']]

plt.plot(grouped_data1['ds'],grouped_data1['y'])

"""Starting with canada"""

m = Prophet()

m.fit(canada)

future = m.make_future_dataframe(periods=12, freq='M')

future.shape

forecast = m.predict(future)

m.plot(forecast)

forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()

m.plot_components(forecast)

from prophet.plot import plot_plotly, plot_components_plotly

plot_plotly(m, forecast)

"""The model was more modest in terms of the estimations versus the actual data, but overall not a bad fit. The seasonality is somewhat evident"""

plot_components_plotly(m, forecast)

"""So using the asm dataset works a lot better for getting a coherent model. I will do the same thing for the US and australia"""

fil_usa = asm[(asm['Country / territory of asylum/residence'] == "USA (INS/DHS)")]

selected_usa = fil_usa[['Origin', 'Year', 'Month', 'Value']]

# Group by 'Origin' and 'Year', and calculate the sum of 'Value'
grouped_usa = selected_usa.groupby(['Year','Month']).agg(Total=('Value', 'sum')).reset_index()
#grouped_usa = grouped_usa.groupby(['Year']).agg(Total=('Total', 'sum')).reset_index() # sum all the countries respective values for that year

grouped_usa.shape
grouped_usa.info

grouped_usa['Month_Num'] = grouped_usa['Month'].map(month_map)

# Combine 'Year' and 'Month_Num' columns to create a new column representing the first day of each month
grouped_usa['First_Day_of_Month'] = pd.to_datetime(grouped_usa['Year'].astype(str) + '-' + grouped_data1['Month_Num'].astype(str) + '-01')

grouped_usa.head()
grouped_usa = grouped_usa.rename(columns={'First_Day_of_Month': "ds", "Total" : "y"})

usa = grouped_usa[['ds','y']]
#usa['y'] = pd.to_datetime(usa['y'])

m1 = Prophet()
m1.fit(usa)

future1 = m1.make_future_dataframe(periods=24, freq='M')
future1.tail()

forecast1 = m1.predict(future1)
forecast1[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()

fig1_a = m1.plot(forecast1)

m1.plot_components(forecast1)

plot_plotly(m1, forecast1)

plot_components_plotly(m1, forecast1)

"""Model performed even better. Trying out australia now"""

filtered_aus = asm[(asm['Country / territory of asylum/residence'] == "Australia")]

selected_aus = filtered_aus[['Origin', 'Year', 'Month', 'Value']]

grouped_aus = selected_aus.groupby(['Year', 'Month']).agg(Total=('Value', 'sum')).reset_index() # sum up the value counts for that month

grouped_aus.shape
grouped_aus.info

# Convert month names to their numerical representation
grouped_aus['Month_Num'] = grouped_aus['Month'].map(month_map)

# Combine 'Year' and 'Month_Num' columns to create a new column representing the first day of each month
grouped_aus['First_Day_of_Month'] = pd.to_datetime(grouped_aus['Year'].astype(str) + '-' + grouped_aus['Month_Num'].astype(str) + '-01')

grouped_aus.head()
grouped_aus = grouped_aus.rename(columns={'First_Day_of_Month': "ds", "Total" : "y"})

aus = grouped_aus[['ds','y']]
plt.plot(aus['ds'], aus['y'])

m2 = Prophet()
m2.fit(aus)

future2 = m2.make_future_dataframe(periods=24 , freq='M')
future2.tail()

forecast2 = m2.predict(future2)
forecast2[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()

fig1_b = m2.plot(forecast2)

m2.plot_components(forecast2)

plot_plotly(m2, forecast2)

plot_components_plotly(m2, forecast2)

"""Figuring out cross validation now"""

from prophet.diagnostics import cross_validation
df_cv = cross_validation(m, initial='1825 days', period='30 days', horizon = '90 days')

df_cv.head()

"""Evaluating the model-- it doesn't look good"""

from prophet.diagnostics import performance_metrics
df_p = performance_metrics(df_cv)
df_p.head()

# usa
df_cv1 = cross_validation(m1, initial='1825 days', period='30 days', horizon = '90 days')
df_p1 = performance_metrics(df_cv1)
df_p1.head()

# australia
df_cv2 = cross_validation(m2, initial='1825 days', period='30 days', horizon = '90 days')
df_p2 = performance_metrics(df_cv2)
df_p2.head()



"""Experimenting with RNN: based on this [link](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)"""

tf.random.set_seed(7)

"""Trying a different code for RNN based on this new [link](https://https://www.section.io/engineering-education/univariate-time-series-using-recurrent-neural-networks/) instead

Canada first again:
"""

from sklearn.preprocessing import MinMaxScaler

canada.shape

# split into train and test sets
train_size1 = int(len(canada) * 0.67)
train_size1

train1 = canada[:148]
test1 = canada[148:]

train1.set_index('ds', inplace=True)
test1.set_index('ds', inplace=True)

scaler = MinMaxScaler()
#scaler = MinMaxScaler(feature_range=(0, 1))
#canada = scaler.fit_transform(canada)

scaler.fit(train1)
scaler.fit(test1)

scaled_train1_dates = scaler.transform(train1)
scaled_test1_dates = scaler.transform(test1)
scaled_train1_dates[1:]

from keras.preprocessing.sequence import TimeseriesGenerator

n_input = 6
n_features = 1
trainY = scaled_train1_dates.reshape((len(scaled_train1_dates), n_features))
testY = scaled_test1_dates.reshape((len(scaled_test1_dates), n_features))

generated_batches = TimeseriesGenerator(trainY, trainY, length=n_input, batch_size=1)

for i in range(len(generated_batches)):
    data, targets = generated_batches[i]

lstm_model = Sequential()
lstm_model.add(LSTM(15, activation='relu', input_shape=(n_input, n_features)))
lstm_model.add(Dense(10, activation='relu')) #, input_shape=(n_input, n_features)))
lstm_model.add(Dense(1))
lstm_model.compile(optimizer='adam', loss='mse')
lstm_model.fit(generated_batches, epochs=10, batch_size=1)
#lstm_model.fit(trainX, trainY, batch_size =1, epochs=1)

prediction_result = []

test_batches = scaled_train1_dates[-n_input:]
reshaping_batches = test_batches.reshape(1, n_input, n_features)

for i in range(len(test1)):

    predicted_output = lstm_model.predict(reshaping_batches)[0]

    prediction_result.append(predicted_output)

    reshaping_batches = np.append(reshaping_batches [:,1:,:],[[predicted_output]],axis=1)

prediction_result

actual_values = scaler.inverse_transform(prediction_result)
test1['Predictions'] = actual_values
test1.plot(figsize=(14,5))

test_generator = TimeseriesGenerator(testY, testY, length=10, batch_size=1)

evaluation = lstm_model.evaluate(test_generator)
print(f"Loss on test data: {evaluation}")

# Obtain predictions for the test data
predictions = lstm_model.predict(test_generator)
predictions.shape

predictions_inv = scaler.inverse_transform(predictions)
actual_values = scaler.inverse_transform(test_generator.targets)
predictions_inv.shape

# Calculate RMSE
#rmse = np.sqrt(mean_squared_error(actual_values, predictions_inv))
#print(f"Root Mean Squared Error (RMSE): {rmse}")

# Visualize actual vs predicted values
plt.figure(figsize=(10, 6))
plt.plot(actual_values, label='Actual')
plt.plot(predictions_inv, label='Predicted')
plt.title('Canada - Actual vs Predicted Values')
plt.xlabel('Time Steps')
plt.ylabel('Value')
plt.legend()
plt.show()

from sklearn.model_selection import train_test_split

train_size2 = int(len(aus) * 0.67)
train_size2

train2 = aus[:148]
test2 = aus[148:]

train2.set_index('ds', inplace=True)
test2.set_index('ds', inplace=True)

scaler.fit(train2)
scaler.fit(test2)

scaled_train2_dates = scaler.transform(train2)
scaled_test2_dates = scaler.transform(test2)
scaled_train2_dates[1:]

trainY2 = scaled_train2_dates.reshape((len(scaled_train2_dates), n_features))
testY2 = scaled_test2_dates.reshape((len(scaled_test2_dates), n_features))

generated_batches2 = TimeseriesGenerator(trainY2, trainY2, length=n_input, batch_size=1)

for i in range(len(generated_batches2)):
    data, targets = generated_batches2[i]

lstm_model2 = Sequential()
lstm_model2.add(LSTM(15, activation='sigmoid', input_shape=(n_input, n_features)))
lstm_model2.add(Dense(15, activation='tanh'))
lstm_model2.add(Dense(1))
lstm_model2.compile(optimizer='adam', loss='mse')
lstm_model2.fit(generated_batches2, epochs=10, batch_size=1)

prediction_result2 = []

test_batches2 = scaled_train2_dates[-n_input:]
reshaping_batches2 = test_batches2.reshape(1, n_input, n_features)

for i in range(len(test2)):

    predicted_output2 = lstm_model2.predict(reshaping_batches2)[0]

    prediction_result2.append(predicted_output2)

    reshaping_batches2 = np.append(reshaping_batches2 [:,1:,:],[[predicted_output2]],axis=1)

actual_values2 = scaler.inverse_transform(prediction_result2)
test2['Predictions'] = actual_values2
test2.plot(figsize=(14,5))

test_generator2 = TimeseriesGenerator(testY2, testY2, length=10, batch_size=1)

evaluation2 = lstm_model2.evaluate(test_generator2)
print(f"Loss on test data: {evaluation2}")

# Obtain predictions for the test data
predictions2 = lstm_model2.predict(test_generator2)
predictions2.shape

predictions_inv2 = scaler.inverse_transform(predictions2)
actual_values2 = scaler.inverse_transform(test_generator2.targets)
predictions_inv2.shape

# Calculate RMSE
#rmse = np.sqrt(mean_squared_error(actual_values, predictions_inv))
#print(f"Root Mean Squared Error (RMSE): {rmse}")

# Visualize actual vs predicted values
plt.figure(figsize=(10, 6))
plt.plot(actual_values2, label='Actual')
plt.plot(predictions_inv2, label='Predicted')
plt.title('Australia - Actual vs Predicted Values')
plt.xlabel('Time Steps')
plt.ylabel('Value')
plt.legend()
plt.show()

"""Looks like there was overfitting in our models so far, and the RNN's may not be able to handle the seasonality of the trends in the data. Also the architecture of the RNNs may not be robust enough (right number of neurons, some mismatched shape issues, issues performing cross validation, etc)"""

train_size3 = int(len(usa) * 0.67)
train_size3

train3 = usa[:140]
test3 = usa[140:]

train3.set_index('ds', inplace=True)
test3.set_index('ds', inplace=True)

scaler.fit(train3)
scaler.fit(test3)

scaled_train3_dates = scaler.transform(train3)
scaled_test3_dates = scaler.transform(test3)
scaled_train3_dates[1:]

trainY3 = scaled_train3_dates.reshape((len(scaled_train3_dates), n_features))
testY3 = scaled_test3_dates.reshape((len(scaled_test3_dates), n_features))

generated_batches3 = TimeseriesGenerator(trainY3, trainY3, length=n_input, batch_size=1)

for i in range(len(generated_batches3)):
    data, targets = generated_batches3[i]

lstm_model3 = Sequential()
lstm_model3.add(LSTM(25, activation='sigmoid', input_shape=(n_input, n_features)))
lstm_model3.add(Dense(10, activation='sigmoid'))
lstm_model3.add(Dense(1))
lstm_model3.compile(optimizer='adam', loss='mse')
lstm_model3.fit(generated_batches2, epochs=10, batch_size=1)

prediction_result3 = []

test_batches3 = scaled_train3_dates[-n_input:]
reshaping_batches3 = test_batches3.reshape(1, n_input, n_features)

for i in range(len(test3)):

    predicted_output3 = lstm_model3.predict(reshaping_batches3)[0]

    prediction_result3.append(predicted_output3)

    reshaping_batches3 = np.append(reshaping_batches3 [:,1:,:],[[predicted_output3]],axis=1)

actual_values3 = scaler.inverse_transform(prediction_result3)
test3['Predictions'] = actual_values3
test3.plot(figsize=(14,5))

test_generator3 = TimeseriesGenerator(testY3, testY3, length=10, batch_size=1)

evaluation3 = lstm_model3.evaluate(test_generator3)
print(f"Loss on test data: {evaluation3}")

# Obtain predictions for the test data
predictions3 = lstm_model3.predict(test_generator3)
predictions3.shape

predictions_inv3 = scaler.inverse_transform(predictions3)
actual_values3 = scaler.inverse_transform(test_generator3.targets)
predictions_inv3.shape

# Calculate RMSE
#rmse = np.sqrt(mean_squared_error(actual_values, predictions_inv))
#print(f"Root Mean Squared Error (RMSE): {rmse}")

# Visualize actual vs predicted values
plt.figure(figsize=(10, 6))
plt.plot(actual_values3, label='Actual')
plt.plot(predictions_inv3, label='Predicted')
plt.title('USA - Actual vs Predicted Values')
plt.xlabel('Time Steps')
plt.ylabel('Value')
plt.legend()
plt.show()